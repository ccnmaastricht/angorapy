{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "In this tutorial we will create a custom gatherer to alter the way in which our agent selects actions. Before we begin, let's quickly go into the gathering process of PPO.\n",
    "\n",
    "During the gathering phase, our agent collects experience via *n* workers. Each of these workers maintains its own copy of the policy as well as its own copy of the environment. The workers are independent: while they share the same version of the policy and environment, random processes are completely independent. Each worker selects an action at every step of the environment. In the standard gathering setup, this selection is purely based on the policy distribution's predicted parameters. Based on these parameters, the gatherer samples the action the agent will execute. We then also determine its probability (based on the PDF or PMF of the distribution) so the optimization can increase or decrease it depending on whether it is advantageous ar disadvantageous.\n",
    "\n",
    "This sampling constitutes a stochastic policy. The advantage of a stochastic policy is its implicit exploration. Because the process of choosing an action is inherently stochastic, no additional care has to be taken of exploration. This advantage, however, comes with its drawbacks: one needs to still assure a good balance between exploration and exploitation. Setting schedules for decreasing the stochasticity requires knowledge about the convergence of the model in the environment. It also lacks flexibility when some spaces in the exploration space still need more exploration than others. Often, one instead lets the model itself predict also the second moment of the policy distribution (e.g. the variance of a Gaussian distribution). To prevent premature convergence, the objective is augmented with an entropy bonus that rewards high exploration. This allows us to use a stochastic policy without explicit exploration mechanisms.\n",
    "\n",
    "In some applications, we may want to deviate from this approach and depend the sampling of actions on other factors than the predicted parameters. Such applications could for instance be models of curiosity. Note that conceptually, PPO is an on-policy algorithm. That means, that the optimization assumes the samples it optimizes on to be directly generated by the policy. However, this is not entirely true in practice. When optimizing, we usually use *mini batch* stochastic gradient descent methods and do multiple epochs per cycle. Naturally, this means that after the first update to the policy, every subsequent update will optimize on experience distributed differently than what the new policy would yield. This indicates some leverage over the gathering process.\n",
    "\n",
    "The following showcases how to do this by means of a simple example: We will add epsilon greedy exploration as an additional exploration mechanism. First, let us import some basic we will need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "import random\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "To implement a custom gatherer, we will extend the ```Gatherer``` class. We do not want to alter the general behaviour of the gatherer (and this is also not recommended) but only change the action selection, and so we overwrite only the method ```select_action(self, predicted_parameters)```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from dexterity.agent.gather import Gatherer\n",
    "\n",
    "class EpsilonGreedyGatherer(Gatherer):\n",
    "\n",
    "    def select_action(self, predicted_parameters: list):\n",
    "        if random.random() < 0.9:\n",
    "            action, action_probability = super(EpsilonGreedyGatherer, self).select_action(predicted_parameters)\n",
    "        else:\n",
    "            action = self.distribution.action_space.sample()\n",
    "            action_probability = self.distribution.log_probability(tf.expand_dims(action, 0), *predicted_parameters)\n",
    "\n",
    "        return action, action_probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The signature is simple: we receive the parameters the policy predicted and return an action and its probability *given those parameters*. The latter we can easily calculate using the distribution of the policy (```self.distribution```).\n",
    "\n",
    "Now that we have a new gatherer, we need to incorporate and train an agent with it. First, we create our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from dexterity.common.transformers import StateNormalizationTransformer\n",
    "from dexterity.common.wrappers import make_env\n",
    "\n",
    "env = make_env(\"CartPole-v1\", transformers=[StateNormalizationTransformer])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Note that in the above we only use the ```StateNormalizationTransformer``` because CartPole's reward function does not cope well with normalization. Next, we create our model and agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using [StateNormalizationTransformer] for preprocessing.\n",
      "An MPI Optimizer with 1 ranks has been created; the following ranks optimize: [0]\n"
     ]
    }
   ],
   "source": [
    "from dexterity.models import get_model_builder\n",
    "from dexterity.agent.ppo_agent import PPOAgent\n",
    "\n",
    "model_builder = get_model_builder(\"simple\", \"ffn\")\n",
    "agent = PPOAgent(\n",
    "    model_builder,\n",
    "    env,\n",
    "    horizon=512,\n",
    "    workers=12\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "For now, this agent would use the default gatherer. Let's change this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "agent.assign_gatherer(EpsilonGreedyGatherer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "That's it. Let's train this thing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Drill started using 1 processes for 6 workers of which 1 are optimizers. Worker distribution: [6].\n",
      "IDs over Workers: [[0, 1, 2, 3, 4, 5]]\n",
      "IDs over Optimizers: [[0, 1, 2, 3, 4, 5]]\n",
      "\u001B[92mBefore Training\u001B[0m: r: \u001B[91m   21.09\u001B[0m; len: \u001B[94m   21.09\u001B[0m; n: \u001B[94m140\u001B[0m; loss: [\u001B[94m-\u001B[0m|\u001B[94m-\u001B[0m|\u001B[94m-\u001B[0m]; eps: \u001B[94m    0\u001B[0m; lr: \u001B[94m1.00e-03\u001B[0m; upd: \u001B[94m     0\u001B[0m; f: \u001B[94m   0.000\u001B[0mk; y.exp: \u001B[94m0.00000\u001B[0m; times:  ; took s [unknown time left]; mem: 1.77/33|0.51/8.36;\n",
      "\u001B[92mCycle     1/20\u001B[0m: r: \u001B[91m   33.85\u001B[0m; len: \u001B[94m   33.85\u001B[0m; n: \u001B[94m 85\u001B[0m; loss: [\u001B[94m  0.04\u001B[0m|\u001B[94m   38.15\u001B[0m|\u001B[94m  0.68\u001B[0m]; eps: \u001B[94m  140\u001B[0m; lr: \u001B[94m1.00e-03\u001B[0m; upd: \u001B[94m   144\u001B[0m; f: \u001B[94m   3.072\u001B[0mk; times: [10.5|0.0|2.7] [79|0|21]; took 13.88s [4.4mins left]; mem: 1.8/33|0.51/8.36;\n",
      "\u001B[92mCycle     2/20\u001B[0m: r: \u001B[91m   69.21\u001B[0m; len: \u001B[94m   69.21\u001B[0m; n: \u001B[94m 38\u001B[0m; loss: [\u001B[94m -0.16\u001B[0m|\u001B[94m   66.15\u001B[0m|\u001B[94m  0.64\u001B[0m]; eps: \u001B[94m  225\u001B[0m; lr: \u001B[94m1.00e-03\u001B[0m; upd: \u001B[94m   288\u001B[0m; f: \u001B[94m   6.144\u001B[0mk; times: [11.0|0.0|2.2] [83|0|17]; took 12.83s [4.0mins left]; mem: 1.8/33|0.51/8.36;\n",
      "\u001B[92mCycle     3/20\u001B[0m: r: \u001B[91m  114.24\u001B[0m; len: \u001B[94m  114.24\u001B[0m; n: \u001B[94m 25\u001B[0m; loss: [\u001B[94m  0.12\u001B[0m|\u001B[94m   86.44\u001B[0m|\u001B[94m  0.61\u001B[0m]; eps: \u001B[94m  263\u001B[0m; lr: \u001B[94m1.00e-03\u001B[0m; upd: \u001B[94m   432\u001B[0m; f: \u001B[94m   9.216\u001B[0mk; times: [10.5|0.0|2.3] [82|0|18]; took 12.44s [3.7mins left]; mem: 1.8/33|0.51/8.36;\n",
      "\u001B[92mCycle     4/20\u001B[0m: r: \u001B[91m  124.72\u001B[0m; len: \u001B[94m  124.72\u001B[0m; n: \u001B[94m 18\u001B[0m; loss: [\u001B[94m -0.00\u001B[0m|\u001B[94m   96.44\u001B[0m|\u001B[94m  0.57\u001B[0m]; eps: \u001B[94m  288\u001B[0m; lr: \u001B[94m1.00e-03\u001B[0m; upd: \u001B[94m   576\u001B[0m; f: \u001B[94m  12.288\u001B[0mk; times: [10.0|0.0|2.2] [82|0|18]; took 12.55s [3.4mins left]; mem: 1.8/33|0.51/8.36;\n",
      "\u001B[92mCycle     5/20\u001B[0m: r: \u001B[91m  177.93\u001B[0m; len: \u001B[94m  177.93\u001B[0m; n: \u001B[94m 14\u001B[0m; loss: [\u001B[94m -0.01\u001B[0m|\u001B[94m   98.51\u001B[0m|\u001B[94m  0.54\u001B[0m]; eps: \u001B[94m  306\u001B[0m; lr: \u001B[94m1.00e-03\u001B[0m; upd: \u001B[94m   720\u001B[0m; f: \u001B[94m  15.360\u001B[0mk; times: [10.1|0.0|2.1] [83|0|17]; took 12.44s [3.2mins left]; mem: 1.8/33|0.51/8.36;\n",
      "\u001B[92mCycle     6/20\u001B[0m: r: \u001B[91m  190.14\u001B[0m; len: \u001B[94m  190.14\u001B[0m; n: \u001B[94m 14\u001B[0m; loss: [\u001B[94m -0.01\u001B[0m|\u001B[94m   97.87\u001B[0m|\u001B[94m  0.54\u001B[0m]; eps: \u001B[94m  320\u001B[0m; lr: \u001B[94m1.00e-03\u001B[0m; upd: \u001B[94m   864\u001B[0m; f: \u001B[94m  18.432\u001B[0mk; times: [10.2|0.0|2.3] [81|0|19]; took 13.12s [3.0mins left]; mem: 1.8/33|0.51/8.36;\n",
      "Optimizing..."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "agent.drill(10, 3, 64)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}